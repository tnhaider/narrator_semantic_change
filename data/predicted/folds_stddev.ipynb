{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the data and extract F1 scores and averages\n",
        "def extract_f1_scores(file_path):\n",
        "    # Initialize variables\n",
        "    results = []\n",
        "    fold_results = []\n",
        "    fold_number = 0\n",
        "    inside_report = False\n",
        "\n",
        "    # Define the label pattern to match the class labels (a, b, c, d) and their F1 scores\n",
        "    label_pattern = re.compile(r\"\\s*([abcd])\\s+\\d+\\.\\d+\\s+\\d+\\.\\d+\\s+([\\d.]+)\\s+\\d+\")\n",
        "    # Define the patterns for f1_macro and f1_weighted averages\n",
        "    f1_macro_pattern = re.compile(r\"macro avg\\s+\\d+\\.\\d+\\s+\\d+\\.\\d+\\s+([\\d.]+)\")\n",
        "    f1_weighted_pattern = re.compile(r\"weighted avg\\s+\\d+\\.\\d+\\s+\\d+\\.\\d+\\s+([\\d.]+)\")\n",
        "\n",
        "    # Open the file and read line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            # Check for the start of a new fold\n",
        "            if line.startswith(\"FOLD\"):\n",
        "                fold_number += 1\n",
        "                inside_report = False\n",
        "\n",
        "            # Check for the start of the report section\n",
        "            if \"report:\" in line:\n",
        "                inside_report = True\n",
        "\n",
        "            # If inside the report section, match the label and F1 score\n",
        "            if inside_report:\n",
        "                match = label_pattern.match(line)\n",
        "                if match:\n",
        "                    label = match.group(1)\n",
        "                    f1_score = float(match.group(2))\n",
        "                    results.append([f'Fold {fold_number}', label, f1_score])\n",
        "                # Match for f1_macro and f1_weighted averages\n",
        "                macro_match = f1_macro_pattern.search(line)\n",
        "                if macro_match:\n",
        "                    f1_macro = float(macro_match.group(1))\n",
        "                    fold_results.append([f'Fold {fold_number}', 'f1_macro', f1_macro])\n",
        "                weighted_match = f1_weighted_pattern.search(line)\n",
        "                if weighted_match:\n",
        "                    f1_weighted = float(weighted_match.group(1))\n",
        "                    fold_results.append([f'Fold {fold_number}', 'f1_weighted', f1_weighted])\n",
        "\n",
        "    # Convert the results to DataFrames\n",
        "    df_classes = pd.DataFrame(results, columns=[\"Fold\", \"Class\", \"F1-Score\"])\n",
        "    df_averages = pd.DataFrame(fold_results, columns=[\"Fold\", \"Metric\", \"Value\"])\n",
        "    return df_classes, df_averages\n",
        "\n",
        "# Path to the data file\n",
        "file_path = \"erzaehler_transferzeit_next_folds.txt\"\n",
        "\n",
        "# Extract F1 scores and averages, then display the DataFrames\n",
        "df_classes, df_averages = extract_f1_scores(file_path)\n",
        "print(\"F1 Scores DataFrame:\")\n",
        "print(df_classes)\n",
        "\n",
        "print(\"\\nF1 Macro and Weighted Averages DataFrame:\")\n",
        "print(df_averages)\n",
        "\n",
        "# Calculate the average and standard deviation for each class\n",
        "summary_classes_df = df_classes.groupby('Class')['F1-Score'].agg(['mean', 'std']).reset_index()\n",
        "summary_classes_df.columns = ['Class', 'Average F1-Score', 'Standard Deviation']\n",
        "\n",
        "# Calculate the average and standard deviation for macro and weighted F1 scores\n",
        "summary_averages_df = df_averages.groupby('Metric')['Value'].agg(['mean', 'std']).reset_index()\n",
        "summary_averages_df.columns = ['Metric', 'Average Value', 'Standard Deviation']\n",
        "\n",
        "print(\"\\nSummary DataFrame for Classes:\")\n",
        "print(summary_classes_df)\n",
        "\n",
        "print(\"\\nSummary DataFrame for Averages:\")\n",
        "print(summary_averages_df)\n",
        "\n",
        "# Save the results to CSV files\n",
        "df_classes.to_csv(\"f1_scores_by_fold.csv\", index=False)\n",
        "df_averages.to_csv(\"f1_averages_by_fold.csv\", index=False)\n",
        "summary_classes_df.to_csv(\"f1_scores_summary_classes.csv\", index=False)\n",
        "summary_averages_df.to_csv(\"f1_averages_summary.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8GS1N83wPlf",
        "outputId": "892ebbc3-9634-4a18-dde6-30d9450d2bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Scores DataFrame:\n",
            "       Fold Class  F1-Score\n",
            "0    Fold 1     a    0.1818\n",
            "1    Fold 1     b    0.8167\n",
            "2    Fold 1     c    0.5714\n",
            "3    Fold 1     d    0.3871\n",
            "4    Fold 2     a    0.0000\n",
            "5    Fold 2     b    0.9104\n",
            "6    Fold 2     c    0.4615\n",
            "7    Fold 2     d    0.7500\n",
            "8    Fold 3     a    0.6000\n",
            "9    Fold 3     b    0.8947\n",
            "10   Fold 3     c    0.4706\n",
            "11   Fold 3     d    0.5714\n",
            "12   Fold 4     a    0.2500\n",
            "13   Fold 4     b    0.7692\n",
            "14   Fold 4     c    0.4762\n",
            "15   Fold 4     d    0.6512\n",
            "16   Fold 5     a    0.4444\n",
            "17   Fold 5     b    0.8571\n",
            "18   Fold 5     c    0.1250\n",
            "19   Fold 5     d    0.6875\n",
            "20   Fold 6     a    0.6667\n",
            "21   Fold 6     b    0.8640\n",
            "22   Fold 6     c    0.2500\n",
            "23   Fold 6     d    0.3846\n",
            "24   Fold 7     a    0.0000\n",
            "25   Fold 7     b    0.8254\n",
            "26   Fold 7     c    0.3077\n",
            "27   Fold 7     d    0.5882\n",
            "28   Fold 8     a    0.6154\n",
            "29   Fold 8     b    0.8644\n",
            "30   Fold 8     c    0.1538\n",
            "31   Fold 8     d    0.5625\n",
            "32   Fold 9     a    0.2857\n",
            "33   Fold 9     b    0.9091\n",
            "34   Fold 9     c    0.6250\n",
            "35   Fold 9     d    0.6250\n",
            "36  Fold 10     a    0.2857\n",
            "37  Fold 10     b    0.8205\n",
            "38  Fold 10     c    0.3636\n",
            "39  Fold 10     d    0.5882\n",
            "\n",
            "F1 Macro and Weighted Averages DataFrame:\n",
            "       Fold       Metric   Value\n",
            "0    Fold 1     f1_macro  0.4893\n",
            "1    Fold 1  f1_weighted  0.6513\n",
            "2    Fold 2     f1_macro  0.5305\n",
            "3    Fold 2  f1_weighted  0.8269\n",
            "4    Fold 3     f1_macro  0.6342\n",
            "5    Fold 3  f1_weighted  0.7626\n",
            "6    Fold 4     f1_macro  0.5366\n",
            "7    Fold 4  f1_weighted  0.6670\n",
            "8    Fold 5     f1_macro  0.5285\n",
            "9    Fold 5  f1_weighted  0.7408\n",
            "10   Fold 6     f1_macro  0.5413\n",
            "11   Fold 6  f1_weighted  0.7292\n",
            "12   Fold 7     f1_macro  0.4303\n",
            "13   Fold 7  f1_weighted  0.6845\n",
            "14   Fold 8     f1_macro  0.5490\n",
            "15   Fold 8  f1_weighted  0.7025\n",
            "16   Fold 9     f1_macro  0.6112\n",
            "17   Fold 9  f1_weighted  0.7942\n",
            "18  Fold 10     f1_macro  0.5145\n",
            "19  Fold 10  f1_weighted  0.6801\n",
            "\n",
            "Summary DataFrame for Classes:\n",
            "  Class  Average F1-Score  Standard Deviation\n",
            "0     a           0.33297            0.242855\n",
            "1     b           0.85315            0.045469\n",
            "2     c           0.38048            0.169755\n",
            "3     d           0.57957            0.117121\n",
            "\n",
            "Summary DataFrame for Averages:\n",
            "        Metric  Average Value  Standard Deviation\n",
            "0     f1_macro        0.53654            0.057150\n",
            "1  f1_weighted        0.72391            0.057542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "data = open('erzaehler_vanilla_folds.txt', 'r').read()\n",
        "data1 = open('erzaehler_down_folds.txt', 'r').read()"
      ],
      "metadata": {
        "id": "6I3Msv2WtQIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "LY4w1MCrsdmh",
        "outputId": "6fd1f634-147d-4037-86f1-fc64fc72dbeb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "bad character range n-f at position 11",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-231d0e23aa4c>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Extract F1 scores and display the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_f1_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-231d0e23aa4c>\u001b[0m in \u001b[0;36mextract_f1_scores\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Define the label pattern to match the class labels (a, b, c, d) and their F1 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlabel_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\s*([fic|non-fic])\\s+\\d+\\.\\d+\\s+\\d+\\.\\d+\\s+([\\d.]+)\\s+\\d+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Open the file and read line by line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    445\u001b[0m                            not nested and not items))\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    839\u001b[0m             sub_verbose = ((verbose or (add_flags & SRE_FLAG_VERBOSE)) and\n\u001b[1;32m    840\u001b[0m                            not (del_flags & SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    445\u001b[0m                            not nested and not items))\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bad character range %s-%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m                     \u001b[0msetappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRANGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: bad character range n-f at position 11"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the data and extract F1 scores\n",
        "def extract_f1_scores(file_path):\n",
        "    # Initialize variables\n",
        "    results = []\n",
        "    fold_number = 0\n",
        "    inside_report = False\n",
        "\n",
        "    # Define the label pattern to match the class labels (a, b, c, d) and their F1 scores\n",
        "    label_pattern = re.compile(r\"\\s*([fic|non-fic])\\s+\\d+\\.\\d+\\s+\\d+\\.\\d+\\s+([\\d.]+)\\s+\\d+\")\n",
        "\n",
        "    # Open the file and read line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            # Check for the start of a new fold\n",
        "            if line.startswith(\"FOLD\"):\n",
        "                fold_number += 1\n",
        "                inside_report = False\n",
        "\n",
        "            # Check for the start of the report section\n",
        "            if \"report:\" in line:\n",
        "                inside_report = True\n",
        "\n",
        "            # If inside the report section, match the label and F1 score\n",
        "            if inside_report:\n",
        "                match = label_pattern.match(line)\n",
        "                if match:\n",
        "                    label = match.group(1)\n",
        "                    f1_score = float(match.group(2))\n",
        "                    results.append([f'Fold {fold_number}', label, f1_score])\n",
        "\n",
        "    # Convert the results to a DataFrame\n",
        "    df = pd.DataFrame(results, columns=[\"Fold\", \"Class\", \"F1-Score\"])\n",
        "    return df\n",
        "\n",
        "# Path to the data file\n",
        "file_path = \"erzaehler_fic_folds.txt\"\n",
        "\n",
        "# Extract F1 scores and display the DataFrame\n",
        "df = extract_f1_scores(file_path)\n",
        "print(df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df.to_csv(\"f1_scores_by_fold.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the average and standard deviation for each class\n",
        "summary_df = df.groupby('Class')['F1-Score'].agg(['mean', 'std']).reset_index()\n",
        "summary_df.columns = ['Class', 'Average F1-Score', 'Standard Deviation']\n",
        "print(\"\\nSummary DataFrame:\")\n",
        "print(summary_df)\n",
        "summary_df.to_csv(\"f1_scores_summary.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYroQF7aua6y",
        "outputId": "81ad275a-c042-4e8c-bc1a-5e35fc28cf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: [Class, Average F1-Score, Standard Deviation]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the data and extract precision, recall, and F1 scores\n",
        "def extract_scores(file_path):\n",
        "    # Initialize variables\n",
        "    results = []\n",
        "    fold_number = 0\n",
        "    inside_report = False\n",
        "\n",
        "    # Define the label pattern to match the class labels (fic, non-fic) and their scores\n",
        "    label_pattern = re.compile(r\"\\s*(fic|non-fic)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+\\d+\")\n",
        "\n",
        "    # Open the file and read line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            # Check for the start of a new fold\n",
        "            if line.startswith(\"FOLD\"):\n",
        "                fold_number += 1\n",
        "                inside_report = False\n",
        "\n",
        "            # Check for the start of the report section\n",
        "            if \"report:\" in line:\n",
        "                inside_report = True\n",
        "\n",
        "            # If inside the report section, match the label and scores\n",
        "            if inside_report:\n",
        "                match = label_pattern.match(line)\n",
        "                if match:\n",
        "                    label = match.group(1)\n",
        "                    precision = float(match.group(2))\n",
        "                    recall = float(match.group(3))\n",
        "                    f1_score = float(match.group(4))\n",
        "                    results.append([f'Fold {fold_number}', label, precision, recall, f1_score])\n",
        "\n",
        "    # Convert the results to a DataFrame\n",
        "    df = pd.DataFrame(results, columns=[\"Fold\", \"Class\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
        "    return df\n",
        "\n",
        "# Path to the data file\n",
        "file_path = \"erzaehler_fic_folds.txt\"\n",
        "\n",
        "# Extract scores and display the DataFrame\n",
        "df = extract_scores(file_path)\n",
        "print(df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df.to_csv(\"scores_by_fold.csv\", index=False)\n",
        "\n",
        "# Calculate the average and standard deviation for each class and each metric\n",
        "summary_df = df.groupby('Class').agg(\n",
        "    Average_Precision=('Precision', 'mean'),\n",
        "    StdDev_Precision=('Precision', 'std'),\n",
        "    Average_Recall=('Recall', 'mean'),\n",
        "    StdDev_Recall=('Recall', 'std'),\n",
        "    Average_F1_Score=('F1-Score', 'mean'),\n",
        "    StdDev_F1_Score=('F1-Score', 'std')\n",
        ").reset_index()\n",
        "\n",
        "print(\"\\nSummary DataFrame:\")\n",
        "print(summary_df)\n",
        "summary_df.to_csv(\"scores_summary.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTTYxWjoY6f",
        "outputId": "feddc450-ea63-4eea-8ae9-ecafbdadccb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Fold    Class  Precision  Recall  F1-Score\n",
            "0    Fold 1      fic     0.6087  0.5833    0.5957\n",
            "1    Fold 1  non-fic     0.8462  0.8594    0.8527\n",
            "2    Fold 2      fic     0.6250  0.8333    0.7143\n",
            "3    Fold 2  non-fic     0.9531  0.8714    0.9104\n",
            "4    Fold 3      fic     0.7778  0.5000    0.6087\n",
            "5    Fold 3  non-fic     0.8000  0.9333    0.8615\n",
            "6    Fold 4      fic     0.7667  0.6571    0.7077\n",
            "7    Fold 4  non-fic     0.7931  0.8679    0.8288\n",
            "8    Fold 5      fic     0.6562  0.8400    0.7368\n",
            "9    Fold 5  non-fic     0.9286  0.8254    0.8739\n",
            "10   Fold 6      fic     0.6667  0.6667    0.6667\n",
            "11   Fold 6  non-fic     0.8955  0.8955    0.8955\n",
            "12   Fold 7      fic     0.7500  0.4000    0.5217\n",
            "13   Fold 7  non-fic     0.7500  0.9310    0.8308\n",
            "14   Fold 8      fic     0.8800  0.8462    0.8627\n",
            "15   Fold 8  non-fic     0.9365  0.9516    0.9440\n",
            "16   Fold 9      fic     0.9048  0.6552    0.7600\n",
            "17   Fold 9  non-fic     0.8507  0.9661    0.9048\n",
            "18  Fold 10      fic     0.8333  0.5357    0.6522\n",
            "19  Fold 10  non-fic     0.8143  0.9500    0.8769\n",
            "\n",
            "Summary DataFrame:\n",
            "     Class  Average_Precision  StdDev_Precision  Average_Recall  \\\n",
            "0      fic            0.74692          0.105455         0.65175   \n",
            "1  non-fic            0.85680          0.069061         0.90516   \n",
            "\n",
            "   StdDev_Recall  Average_F1_Score  StdDev_F1_Score  \n",
            "0       0.153128           0.68265         0.096077  \n",
            "1       0.047630           0.87793         0.036529  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the data and extract precision, recall, and F1 scores\n",
        "def extract_scores(file_path):\n",
        "    # Initialize variables\n",
        "    results = []\n",
        "    fold_number = 0\n",
        "    inside_report = False\n",
        "\n",
        "    # Define the label pattern to match the class labels (fic, non-fic) and their scores\n",
        "    label_pattern = re.compile(r\"\\s*(fic|non-fic)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+\\d+\")\n",
        "    macro_avg_pattern = re.compile(r\"\\s*macro avg\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+\\d+\")\n",
        "\n",
        "    # Open the file and read line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            # Check for the start of a new fold\n",
        "            if line.startswith(\"FOLD\"):\n",
        "                fold_number += 1\n",
        "                inside_report = False\n",
        "\n",
        "            # Check for the start of the report section\n",
        "            if \"report:\" in line:\n",
        "                inside_report = True\n",
        "\n",
        "            # If inside the report section, match the label and scores\n",
        "            if inside_report:\n",
        "                label_match = label_pattern.match(line)\n",
        "                macro_match = macro_avg_pattern.match(line)\n",
        "                if label_match:\n",
        "                    label = label_match.group(1)\n",
        "                    precision = float(label_match.group(2))\n",
        "                    recall = float(label_match.group(3))\n",
        "                    f1_score = float(label_match.group(4))\n",
        "                    results.append([f'Fold {fold_number}', label, precision, recall, f1_score])\n",
        "                elif macro_match:\n",
        "                    macro_f1_score = float(macro_match.group(3))\n",
        "                    results.append([f'Fold {fold_number}', 'macro avg', None, None, macro_f1_score])\n",
        "\n",
        "    # Convert the results to a DataFrame\n",
        "    df = pd.DataFrame(results, columns=[\"Fold\", \"Class\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
        "    return df\n",
        "\n",
        "# Path to the data file\n",
        "file_path = \"erzaehler_fic_folds.txt\"\n",
        "\n",
        "# Extract scores and display the DataFrame\n",
        "df = extract_scores(file_path)\n",
        "print(df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df.to_csv(\"scores_by_fold.csv\", index=False)\n",
        "\n",
        "# Calculate the average and standard deviation for each class and each metric\n",
        "summary_df = df.groupby('Class').agg(\n",
        "    Average_Precision=('Precision', 'mean'),\n",
        "    StdDev_Precision=('Precision', 'std'),\n",
        "    Average_Recall=('Recall', 'mean'),\n",
        "    StdDev_Recall=('Recall', 'std'),\n",
        "    Average_F1_Score=('F1-Score', 'mean'),\n",
        "    StdDev_F1_Score=('F1-Score', 'std')\n",
        ").reset_index()\n",
        "\n",
        "# Handle macro avg separately\n",
        "macro_df = df[df['Class'] == 'macro avg']\n",
        "macro_summary = macro_df['F1-Score'].agg(\n",
        "    Average_Macro_F1_Score='mean',\n",
        "    StdDev_Macro_F1_Score='std'\n",
        ")\n",
        "\n",
        "# Add macro avg to summary_df\n",
        "summary_df = summary_df.append({\n",
        "    'Class': 'macro avg',\n",
        "    'Average_Precision': None,\n",
        "    'StdDev_Precision': None,\n",
        "    'Average_Recall': None,\n",
        "    'StdDev_Recall': None,\n",
        "    'Average_F1_Score': macro_summary['Average_Macro_F1_Score'],\n",
        "    'StdDev_F1_Score': macro_summary['StdDev_Macro_F1_Score']\n",
        "}, ignore_index=True)\n",
        "\n",
        "print(\"\\nSummary DataFrame:\")\n",
        "print(summary_df)\n",
        "summary_df.to_csv(\"scores_summary.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "G_3sBVnY_Ysa",
        "outputId": "0a6ca8fb-c5fd-4e59-8fdd-0b34596b0023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Fold      Class  Precision  Recall  F1-Score\n",
            "0    Fold 1        fic     0.6087  0.5833    0.5957\n",
            "1    Fold 1    non-fic     0.8462  0.8594    0.8527\n",
            "2    Fold 1  macro avg        NaN     NaN    0.7242\n",
            "3    Fold 2        fic     0.6250  0.8333    0.7143\n",
            "4    Fold 2    non-fic     0.9531  0.8714    0.9104\n",
            "5    Fold 2  macro avg        NaN     NaN    0.8124\n",
            "6    Fold 3        fic     0.7778  0.5000    0.6087\n",
            "7    Fold 3    non-fic     0.8000  0.9333    0.8615\n",
            "8    Fold 3  macro avg        NaN     NaN    0.7351\n",
            "9    Fold 4        fic     0.7667  0.6571    0.7077\n",
            "10   Fold 4    non-fic     0.7931  0.8679    0.8288\n",
            "11   Fold 4  macro avg        NaN     NaN    0.7683\n",
            "12   Fold 5        fic     0.6562  0.8400    0.7368\n",
            "13   Fold 5    non-fic     0.9286  0.8254    0.8739\n",
            "14   Fold 5  macro avg        NaN     NaN    0.8054\n",
            "15   Fold 6        fic     0.6667  0.6667    0.6667\n",
            "16   Fold 6    non-fic     0.8955  0.8955    0.8955\n",
            "17   Fold 6  macro avg        NaN     NaN    0.7811\n",
            "18   Fold 7        fic     0.7500  0.4000    0.5217\n",
            "19   Fold 7    non-fic     0.7500  0.9310    0.8308\n",
            "20   Fold 7  macro avg        NaN     NaN    0.6763\n",
            "21   Fold 8        fic     0.8800  0.8462    0.8627\n",
            "22   Fold 8    non-fic     0.9365  0.9516    0.9440\n",
            "23   Fold 8  macro avg        NaN     NaN    0.9034\n",
            "24   Fold 9        fic     0.9048  0.6552    0.7600\n",
            "25   Fold 9    non-fic     0.8507  0.9661    0.9048\n",
            "26   Fold 9  macro avg        NaN     NaN    0.8324\n",
            "27  Fold 10        fic     0.8333  0.5357    0.6522\n",
            "28  Fold 10    non-fic     0.8143  0.9500    0.8769\n",
            "29  Fold 10  macro avg        NaN     NaN    0.7645\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-41e454bb2222>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Add macro avg to summary_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m summary_df = summary_df.append({\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;34m'Class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'macro avg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m'Average_Precision'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    }
  ]
}